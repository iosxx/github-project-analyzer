#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„ GitHub ä»“åº“åˆ†æè„šæœ¬
ä¸ä¾èµ– RepoMetaAgentï¼Œç›´æ¥ä½¿ç”¨ GitHub API å’Œ OpenAI API
"""
import os
import sys
import json
import requests
from datetime import datetime

# GitHub API é…ç½®
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")
GITHUB_HEADERS = {
    "Accept": "application/vnd.github.v3+json",
}
if GITHUB_TOKEN:
    GITHUB_HEADERS["Authorization"] = f"token {GITHUB_TOKEN}"

# OpenAI API é…ç½®
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_API_BASE = os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4-turbo-preview")


def parse_repo_url(url: str) -> tuple:
    """è§£æ GitHub ä»“åº“ URL"""
    url = url.rstrip("/").replace(".git", "")
    parts = url.split("/")
    owner = parts[-2]
    repo = parts[-1]
    return owner, repo


def fetch_repo_info(owner: str, repo: str) -> dict:
    """è·å–ä»“åº“åŸºæœ¬ä¿¡æ¯"""
    url = f"https://api.github.com/repos/{owner}/{repo}"
    response = requests.get(url, headers=GITHUB_HEADERS)
    response.raise_for_status()
    return response.json()


def fetch_readme(owner: str, repo: str) -> str:
    """è·å– README å†…å®¹"""
    url = f"https://api.github.com/repos/{owner}/{repo}/readme"
    response = requests.get(url, headers=GITHUB_HEADERS)
    if response.status_code == 200:
        import base64
        content = response.json().get("content", "")
        try:
            return base64.b64decode(content).decode("utf-8")
        except:
            return ""
    return ""


def fetch_file_tree(owner: str, repo: str, max_depth: int = 3) -> dict:
    """è·å–æ–‡ä»¶æ ‘ç»“æ„ï¼ˆé™åˆ¶æ·±åº¦é¿å…é€Ÿç‡é™åˆ¶ï¼‰"""
    url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/HEAD?recursive=1"
    response = requests.get(url, headers=GITHUB_HEADERS)
    if response.status_code != 200:
        return {}
    
    data = response.json()
    tree = {}
    
    for item in data.get("tree", [])[:100]:  # é™åˆ¶æ–‡ä»¶æ•°é‡
        path = item["path"]
        parts = path.split("/")
        if len(parts) <= max_depth:
            tree[path] = item["type"]
    
    return tree


def fetch_languages(owner: str, repo: str) -> dict:
    """è·å–ä»“åº“è¯­è¨€ç»Ÿè®¡"""
    url = f"https://api.github.com/repos/{owner}/{repo}/languages"
    response = requests.get(url, headers=GITHUB_HEADERS)
    if response.status_code == 200:
        return response.json()
    return {}


def call_openai(prompt: str, system_prompt: str = "") -> str:
    """è°ƒç”¨ OpenAI API"""
    if not OPENAI_API_KEY:
        return "æœªé…ç½® OpenAI API Key"
    
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": OPENAI_MODEL,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 2000
    }
    
    try:
        response = requests.post(
            f"{OPENAI_API_BASE}/chat/completions",
            headers=headers,
            json=data,
            timeout=120
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except Exception as e:
        print(f"OpenAI API è°ƒç”¨å¤±è´¥: {e}")
        return f"AI åˆ†æå¤±è´¥: {str(e)}"


def analyze_repository(repo_url: str) -> dict:
    """åˆ†æ GitHub ä»“åº“"""
    print(f"ğŸ“Š å¼€å§‹åˆ†æä»“åº“: {repo_url}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {OPENAI_MODEL}")
    print(f"ğŸ”— API ç«¯ç‚¹: {OPENAI_API_BASE}")
    
    owner, repo = parse_repo_url(repo_url)
    print(f"ğŸ“ ä»“åº“: {owner}/{repo}")
    
    # è·å–ä»“åº“ä¿¡æ¯
    print("ğŸ“¥ è·å–ä»“åº“ä¿¡æ¯...")
    try:
        repo_info = fetch_repo_info(owner, repo)
    except Exception as e:
        print(f"âŒ è·å–ä»“åº“ä¿¡æ¯å¤±è´¥: {e}")
        return generate_fallback_results(repo_url, owner, repo, str(e))
    
    # è·å– README
    print("ğŸ“„ è·å– README...")
    readme = fetch_readme(owner, repo)
    
    # è·å–æ–‡ä»¶ç»“æ„
    print("ğŸ“‚ è·å–æ–‡ä»¶ç»“æ„...")
    file_tree = fetch_file_tree(owner, repo)
    
    # è·å–è¯­è¨€ç»Ÿè®¡
    print("ğŸ’» è·å–è¯­è¨€ç»Ÿè®¡...")
    languages = fetch_languages(owner, repo)
    
    # å‡†å¤‡åˆ†æä¸Šä¸‹æ–‡
    context = f"""
ä»“åº“åç§°: {repo_info.get('name', repo)}
æè¿°: {repo_info.get('description', 'æ— æè¿°')}
ä¸»è¦è¯­è¨€: {repo_info.get('language', 'æœªçŸ¥')}
Star æ•°: {repo_info.get('stargazers_count', 0)}
Fork æ•°: {repo_info.get('forks_count', 0)}
ä¸»é¢˜æ ‡ç­¾: {', '.join(repo_info.get('topics', []))}
è®¸å¯è¯: {repo_info.get('license', {}).get('name', 'æœªçŸ¥') if repo_info.get('license') else 'æœªçŸ¥'}

è¯­è¨€ç»Ÿè®¡: {json.dumps(languages, ensure_ascii=False)}

æ–‡ä»¶ç»“æ„ (éƒ¨åˆ†):
{json.dumps(list(file_tree.keys())[:50], ensure_ascii=False, indent=2)}

README å†…å®¹ (å‰ 3000 å­—ç¬¦):
{readme[:3000] if readme else 'æ—  README'}
"""
    
    # AI åˆ†æ
    print("ğŸ§  AI åˆ†æä¸­...")
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ GitHub é¡¹ç›®åˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„ä»“åº“ä¿¡æ¯ï¼Œç”Ÿæˆè¯¦ç»†çš„é¡¹ç›®åˆ†ææŠ¥å‘Šã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œåˆ†æè¦å®¢è§‚ã€ä¸“ä¸šã€æœ‰æ·±åº¦ã€‚"""
    
    analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹ GitHub ä»“åº“å¹¶æä¾›è¯¦ç»†æŠ¥å‘Šï¼š

{context}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›åˆ†æï¼ˆæ¯ä¸ªéƒ¨åˆ†ç”¨ ### æ ‡é¢˜åˆ†éš”ï¼‰ï¼š

### é¡¹ç›®æ¦‚è¿°
ï¼ˆ200-300å­—çš„é¡¹ç›®ä»‹ç»ï¼‰

### æ ¸å¿ƒåŠŸèƒ½
ï¼ˆåˆ—å‡º 3-5 ä¸ªä¸»è¦åŠŸèƒ½ï¼‰

### æŠ€æœ¯ç‰¹ç‚¹
ï¼ˆåˆ†ææŠ€æœ¯æ ˆå’Œæ¶æ„ç‰¹ç‚¹ï¼‰

### ä¼˜ç‚¹
ï¼ˆåˆ—å‡º 3-5 ä¸ªä¼˜ç‚¹ï¼‰

### ä¸è¶³ä¸æ”¹è¿›å»ºè®®
ï¼ˆåˆ—å‡º 2-3 ä¸ªå¯æ”¹è¿›çš„åœ°æ–¹ï¼‰

### é€‚ç”¨åœºæ™¯
ï¼ˆè¯´æ˜é¡¹ç›®é€‚åˆçš„ä½¿ç”¨åœºæ™¯ï¼‰
"""
    
    ai_response = call_openai(analysis_prompt, system_prompt)
    
    # è§£æ AI å“åº”
    sections = parse_ai_response(ai_response)
    
    # æå–å…³é”®è¯
    topics = repo_info.get('topics', [])
    if not topics:
        topics = list(languages.keys())[:5] if languages else []
    
    # æ„å»ºç»“æœ
    results = {
        "long_summary": sections.get("é¡¹ç›®æ¦‚è¿°", repo_info.get('description', 'æš‚æ— æè¿°')),
        "short_summary": sections.get("æ ¸å¿ƒåŠŸèƒ½", "è¯·æŸ¥çœ‹é¡¹ç›® README"),
        "review_report": f"{sections.get('ä¼˜ç‚¹', '')}\n\n{sections.get('ä¸è¶³ä¸æ”¹è¿›å»ºè®®', '')}",
        "keywords": topics + list(languages.keys())[:5],
        "github_topics": topics if topics else list(languages.keys())[:5],
        "file_structure": file_tree,
        "missing_documentation": extract_missing_docs(sections.get("ä¸è¶³ä¸æ”¹è¿›å»ºè®®", "")),
        "suggested_title": sections.get("é€‚ç”¨åœºæ™¯", f"ä½¿ç”¨ {repo} æå‡å¼€å‘æ•ˆç‡"),
        "tech_analysis": sections.get("æŠ€æœ¯ç‰¹ç‚¹", ""),
        "repo_info": {
            "name": repo_info.get('name'),
            "full_name": repo_info.get('full_name'),
            "description": repo_info.get('description'),
            "stars": repo_info.get('stargazers_count'),
            "forks": repo_info.get('forks_count'),
            "language": repo_info.get('language'),
            "topics": topics,
            "license": repo_info.get('license', {}).get('name') if repo_info.get('license') else None
        }
    }
    
    print("âœ… åˆ†æå®Œæˆï¼")
    return results


def parse_ai_response(response: str) -> dict:
    """è§£æ AI å“åº”ä¸ºå„ä¸ªéƒ¨åˆ†"""
    sections = {}
    current_section = None
    current_content = []
    
    for line in response.split('\n'):
        if line.startswith('### '):
            if current_section:
                sections[current_section] = '\n'.join(current_content).strip()
            current_section = line[4:].strip()
            current_content = []
        elif current_section:
            current_content.append(line)
    
    if current_section:
        sections[current_section] = '\n'.join(current_content).strip()
    
    return sections


def extract_missing_docs(text: str) -> list:
    """ä»æ”¹è¿›å»ºè®®ä¸­æå–ç¼ºå¤±æ–‡æ¡£"""
    docs = []
    keywords = ['æ–‡æ¡£', 'è¯´æ˜', 'README', 'API', 'ç¤ºä¾‹', 'æ•™ç¨‹', 'æ³¨é‡Š']
    for keyword in keywords:
        if keyword.lower() in text.lower():
            docs.append(f"å®Œå–„{keyword}")
    return docs if docs else ["æš‚æ— æ˜æ˜¾ç¼ºå¤±"]


def generate_fallback_results(repo_url: str, owner: str, repo: str, error: str) -> dict:
    """ç”Ÿæˆå¤‡ç”¨ç»“æœï¼ˆå½“è·å–ä»“åº“ä¿¡æ¯å¤±è´¥æ—¶ï¼‰"""
    return {
        "long_summary": f"æ— æ³•è·å–ä»“åº“ {owner}/{repo} çš„è¯¦ç»†ä¿¡æ¯ã€‚é”™è¯¯: {error}",
        "short_summary": f"{repo} æ˜¯ä¸€ä¸ª GitHub é¡¹ç›®",
        "review_report": "ç”±äº API é™åˆ¶ï¼Œæ— æ³•å®Œæˆå®Œæ•´åˆ†æ",
        "keywords": [repo, owner, "github"],
        "github_topics": [],
        "file_structure": {},
        "missing_documentation": ["éœ€è¦æ‰‹åŠ¨æŸ¥çœ‹é¡¹ç›®"],
        "suggested_title": f"æ¢ç´¢ {repo} é¡¹ç›®"
    }


def main():
    repo_url = os.environ.get("REPO_URL", "")
    if not repo_url:
        print("é”™è¯¯: æœªæä¾› REPO_URL")
        sys.exit(1)
    
    # è¿è¡Œåˆ†æ
    results = analyze_repository(repo_url)
    
    # ä¿å­˜ç»“æœ
    with open("/tmp/analysis_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜å…ƒæ•°æ®
    meta = {
        "repo_url": repo_url,
        "analyzed_at": datetime.now().isoformat(),
        "title": os.environ.get("ARTICLE_TITLE", ""),
        "openai_model": OPENAI_MODEL,
        "openai_api_base": OPENAI_API_BASE
    }
    with open("/tmp/analysis_meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ° /tmp/analysis_results.json")


if __name__ == "__main__":
    main()
